{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------+---+----+------+----+-------------------+-------+\n",
      "|device_id|         device_name|humidity|lat|long| scale|temp|          timestamp|zipcode|\n",
      "+---------+--------------------+--------+---+----+------+----+-------------------+-------+\n",
      "|        1|sensor-mac-aAt85bOBM|      80| 81|  57|Celius|  30|1.447975123509765E9|  95353|\n",
      "|        2|sensor-mac-able9b...|      36| 13|  56|Celius|  14|1.447975124005187E9|  96484|\n",
      "|        3|sensor-mac-aboutR...|      30| 35|  54|Celius|  17|1.447975124054221E9|  96402|\n",
      "|        4|sensor-mac-across...|      35| 36|  40|Celius|   6|1.447975124102157E9|  96025|\n",
      "|        5|sensor-mac-afterE...|      62| 90|  91|Celius|  23|1.447975124150419E9|  95638|\n",
      "|        6|sensor-mac-allBMO...|      97| 76|  77|Celius|   8|1.447975124197694E9|  95478|\n",
      "|        7|sensor-mac-almost...|      30| 95|  76|Celius|   5|1.447975124246103E9|  95499|\n",
      "|        8|sensor-mac-alsosT...|      74| 95|  79|Celius|  25|1.447975124291892E9|  94857|\n",
      "|        9|sensor-mac-am8CUl...|      93| 37|  29|Celius|  24|1.447975124349377E9|  96149|\n",
      "|       10|sensor-mac-amongj...|      67| 32|  31|Celius|   6|1.447975124399264E9|  96531|\n",
      "|       11|sensor-mac-anKbFx...|      77| 68|  44|Celius|   8|1.447975124448621E9|  95850|\n",
      "|       12|sensor-mac-andeKY...|      96| 98|  87|Celius|  23|1.447975124498138E9|  95524|\n",
      "|       13|sensor-mac-anyf4C...|      54| 78|  14|Celius|  33|1.447975124546061E9|  96025|\n",
      "|       14|sensor-mac-arevx9...|      83| 64|  90|Celius|  29|1.447975124596101E9|  95470|\n",
      "|       15|sensor-mac-asZWyp...|      89|  6|  70|Celius|  16|1.447975124641159E9|  96756|\n",
      "|       16|sensor-mac-atV9GY...|      73| 73|  13|Celius|  32|1.447975124685129E9|  96846|\n",
      "|       17|sensor-mac-be8Ab7...|      53| 29|  54|Celius|  25|1.447975124734084E9|  95323|\n",
      "|       18|sensor-mac-becaus...|      97|  8|  60|Celius|  15|1.447975124783517E9|  96577|\n",
      "|       19|sensor-mac-beenTi...|      62| 67|  72|Celius|  31|1.447975124832391E9|  96358|\n",
      "|       20|sensor-mac-butYnx...|      29| 57|  16|Celius|   5|1.447975124881703E9|  96669|\n",
      "+---------+--------------------+--------+---+----+------+----+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import requests\n",
    "import json\n",
    "df = spark.read.json(\"file:///C:/hdaoopdata/devices.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "|Age| Name|destination|source|\n",
      "+---+-----+-----------+------+\n",
      "| 22|David|     London| Paris|\n",
      "| 22|Steve|   New York|Sydney|\n",
      "+---+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, date\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Puvi') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(Name='David', Age=22, destination='London', source='Paris'),\n",
    "    Row(Name='Steve', Age=22, destination='New York', source='Sydney')  \n",
    "    ])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|      Stats|              Value|\n",
      "+-----------+-------------------+\n",
      "|Co-variance|0.09802782486048531|\n",
      "|Correlation|                1.0|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "df = spark.range(0, 10).withColumn(\"rand1\", rand(seed=10)).withColumn(\"rand2\", rand(seed=10))\n",
    "p=df.stat.cov(\"rand1\", \"rand2\")\n",
    "q=df.stat.corr(\"rand1\", \"rand2\")\n",
    "from pyspark.sql import *\n",
    "Result = Row(\"Stats\",\"Value\")\n",
    "r1= Result(\"Co-variance\",p)\n",
    "r2= Result(\"Correlation\",q)\n",
    "ResultData=[r1,r2]\n",
    "df=spark.createDataFrame(ResultData)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| ID|Name|Age|\n",
      "+---+----+---+\n",
      "|  4|Mark| 21|\n",
      "|  2|Luke| 21|\n",
      "|  3| Leo| 24|\n",
      "|  1|Jack| 22|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"1\", \"Jack\",22,\"Data Science\"),\n",
    " (\"2\",\"Luke\",21,\"Data Analytics\"),\n",
    " (\"3\", \"Leo\",24,\"Micro Services\"),\n",
    " (\"4\", \"Mark\",21,\"Data Analytics\")]\n",
    "columns = [\"ID\",\"Name\",\"Age\",\"AreaofInterest\"]\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "#df.show()\n",
    "#agdf = df.describe('Age')\n",
    "#agdf.coalesce(1).write.parquet(\"Age\")\n",
    "#df.orderBy(desc(\"ID\"),desc(\"Name\"),desc(\"Age\")).show()\n",
    "\n",
    "ds = df.select(col(\"ID\"),col(\"Name\"),col(\"Age\"))\n",
    "\n",
    "d = ds.orderBy(desc(\"Name\"))\n",
    "\n",
    "\n",
    "d.show()\n",
    "\n",
    "\n",
    "#df.coalesce(1).write.parquet(\"NameSorted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+-----------------------------------------------+\n",
      "|Intensity|seq |value|date                                           |\n",
      "+---------+----+-----+-----------------------------------------------+\n",
      "|High     |1   |5.0  |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Medium   |0   |3.0  |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Low      |0   |null |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Low      |0   |2.3  |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Medium   |1   |2    |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Medium   |1   |5.0  |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Low      |0   |5.0  |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|High     |5.0 |0    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|High     |8.0 |1    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Low      |0   |null |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Low      |5.6 |1    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Medium   |6   |0    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Low      |6.0 |1    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Low      |0.0 |1    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Medium   |1.0 |0    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Low      |1.0 |1    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|High     |null|1    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Low      |5.6 |1    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Medium   |6   |0    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Medium   |8.0 |1    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Low      |null|null |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Medium   |1   |5.0  |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Low      |0   |null |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Low      |0   |null |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Low      |0   |2.3  |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|null     |1   |3.6  |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|High     |1   |5.0  |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Low      |0   |5.0  |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Medium   |0   |5.0  |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Low      |0   |null |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Low      |1   |null |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|High     |0   |2.3  |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Low      |1   |3.6  |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|null     |0   |null |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Low      |0   |5.0  |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "+---------+----+-----+-----------------------------------------------+\n",
      "\n",
      "+----------+---------+------------------+\n",
      "|      date|Intensity|        Mean_value|\n",
      "+----------+---------+------------------+\n",
      "|2021-01-01|     High|               5.0|\n",
      "|2021-01-01|      Low|              3.65|\n",
      "|2021-01-01|   Medium|3.3333333333333335|\n",
      "|2021-01-02|     High|               0.5|\n",
      "|2021-01-02|      Low|               1.0|\n",
      "|2021-01-02|   Medium|               0.0|\n",
      "|2021-01-03|     High|               1.0|\n",
      "|2021-01-03|      Low|               1.0|\n",
      "|2021-01-03|   Medium|0.3333333333333333|\n",
      "|2021-01-04|     null|               3.6|\n",
      "|2021-01-04|     High|               5.0|\n",
      "|2021-01-04|      Low|              3.65|\n",
      "|2021-01-04|   Medium|               5.0|\n",
      "|2021-01-05|     null|              null|\n",
      "|2021-01-05|     High|               2.3|\n",
      "|2021-01-05|      Low|               4.3|\n",
      "|2021-01-05|   Medium|               5.0|\n",
      "+----------+---------+------------------+\n",
      "\n",
      "Merged Schema\n",
      "Merged Datarame where EMPAGE,EMPDEPT WERE ADDED AFER EMPID,EMPNAME,SALARY followed by your day column\n",
      "+----------+---------+------------------+\n",
      "|date      |Intensity|Mean_value        |\n",
      "+----------+---------+------------------+\n",
      "|2021-01-01|Medium   |3.3333333333333335|\n",
      "|2021-01-02|Medium   |0.0               |\n",
      "|2021-01-03|Medium   |0.3333333333333333|\n",
      "|2021-01-04|Medium   |5.0               |\n",
      "|2021-01-05|Medium   |5.0               |\n",
      "|2021-01-01|High     |5.0               |\n",
      "|2021-01-02|High     |0.5               |\n",
      "|2021-01-03|High     |1.0               |\n",
      "|2021-01-04|High     |5.0               |\n",
      "|2021-01-05|High     |2.3               |\n",
      "|2021-01-01|Low      |3.65              |\n",
      "|2021-01-02|Low      |1.0               |\n",
      "|2021-01-03|Low      |1.0               |\n",
      "|2021-01-04|Low      |3.65              |\n",
      "|2021-01-05|Low      |4.3               |\n",
      "|2021-01-04|null     |3.6               |\n",
      "|2021-01-05|null     |null              |\n",
      "+----------+---------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "def filename(path):\n",
    "    return path\n",
    "\n",
    "sourceFile = udf(filename, StringType())\n",
    "#dfn = spark.read.csv(\"file:///C:/hdaoopdata/ica/\").select(sourceFile(input_file_name()))\n",
    "\n",
    "#dfn.show(truncate=False)\n",
    "\n",
    "#df = spark.read.options(header='True', delimiter=',',mergeShema='True').csv(\"file:///C:/hdaoopdata/ica/\"). \\\n",
    "#    select(col(\"Intensity\"),col(\"seq\"),col(\"value\"),(sourceFile(input_file_name()))). \\\n",
    "#    withColumnRenamed(\"filename(input_file_name())\",\"date\")\n",
    "\n",
    "\n",
    "#max_mtime = 0\n",
    "#for dirname,subdirs,files in os.walk(\"C:/hdaoopdata/ica/\"):\n",
    "#    for fname in files:\n",
    "#        full_path = os.path.join(dirname, fname)\n",
    "#        mtime = os.stat(full_path).st_mtime\n",
    "#        if mtime > max_mtime:\n",
    "#            max_mtime = mtime\n",
    "#            max_dir = dirname\n",
    "#            max_file = fname\n",
    "\n",
    "#print (max_file)\n",
    "\n",
    "\n",
    "df = spark.read.options(header='True',delimiter=',',mergeShema='True',badRecordsPath='file:///C:/hdaoopdata/ica_bad/').\\\n",
    "    csv(\"file:///C:/hdaoopdata/ica_source/\")\n",
    "dff = df.select(df['*'],(sourceFile(input_file_name()))).withColumnRenamed(\"filename(input_file_name())\",\"date\")\n",
    "dff.show(truncate=False,n=100)   \n",
    "\n",
    "dff = dff.withColumn('Month', regexp_extract(dff['date'], '([0-9][0-9][0-9][0-9])-([0-9][0-9])-([0-9][0-9])', 2))\n",
    "dff = dff.withColumn('Year', regexp_extract(dff['date'], '([0-9][0-9][0-9][0-9])-([0-9][0-9])-([0-9][0-9])', 1))\n",
    "dff = dff.withColumn('Day', regexp_extract(dff['date'], '([0-9][0-9][0-9][0-9])-([0-9][0-9])-([0-9][0-9])', 3))\n",
    "\n",
    "df3=dff.select(concat_ws('-',dff.Year,dff.Month,dff.Day)\n",
    "              .alias(\"date\"),col(\"Intensity\"),col(\"seq\"),col(\"value\")) \n",
    "#.show()\n",
    "df3.registerTempTable(\"source_table\")\n",
    "\n",
    "#df4 = df3.groupby('date','Intensity').agg({'value': 'mean'}).show()\n",
    "\n",
    "temp_avg = df3.groupby('date', 'Intensity')\\\n",
    "            .agg(avg('value').alias('Mean_value'))\\\n",
    "           .orderBy(col('date'), col('Intensity')).cache()\n",
    "temp_avg.show()\n",
    "\n",
    "temp_avg.write.mode('overwrite').option(\"mergeSchema\", \"true\").parquet(\"file:///C:/hdaoopdata/ica_target/\")\n",
    "\n",
    "temp_avg.printSchema\n",
    "\n",
    "# Read the partitioned table\n",
    "\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"file:///C:/hdaoopdata/ica_target/\")\n",
    "print(\"Merged Schema\")\n",
    "mergedDF.printSchema\n",
    "print(\"Merged Datarame where EMPAGE,EMPDEPT WERE ADDED AFER EMPID,EMPNAME,SALARY followed by your day column\")\n",
    "mergedDF.show(truncate=False)\n",
    "  \n",
    "\n",
    "#dffq.show(truncate=False)\n",
    "dff.count()\n",
    "mergedDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+--------+-----------------------------------------------+\n",
      "|Intensity|seq  |value|newcolum|date                                           |\n",
      "+---------+-----+-----+--------+-----------------------------------------------+\n",
      "|Medium   |0    |5    |puvi    |file:///C:/hdaoopdata/ica_source/2021-01-06.csv|\n",
      "|Low      |0    |#$   |raja    |file:///C:/hdaoopdata/ica_source/2021-01-06.csv|\n",
      "|Low      |1    |$%%% |tamil   |file:///C:/hdaoopdata/ica_source/2021-01-06.csv|\n",
      "|High     |0    |2.3  |null    |file:///C:/hdaoopdata/ica_source/2021-01-06.csv|\n",
      "|Low      |1    |3.6  |null    |file:///C:/hdaoopdata/ica_source/2021-01-06.csv|\n",
      "|null     |0    |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-06.csv|\n",
      "|Low      |0    |5    |null    |file:///C:/hdaoopdata/ica_source/2021-01-06.csv|\n",
      "|High     |1    |5.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Medium   |0    |3.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Low      |0    |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Low      |0    |2.3  |null    |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Medium   |1    |2    |null    |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Medium   |1    |5.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|Low      |0    |5.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-01.csv|\n",
      "|High     |5.0  |0    |null    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|High     |8.0  |1    |null    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Low      |0    |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Low      |5.6  |1    |null    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Medium   |6    |0    |null    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Low      |6.0  |1    |null    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Low      |0.0  |1    |null    |file:///C:/hdaoopdata/ica_source/2021-01-02.csv|\n",
      "|Medium   |1.0  |0    |null    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Low      |1.0  |1    |null    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|High     |null |1    |null    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Low      |5.6  |1    |null    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Medium   |6    |0    |null    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Medium   |8.0  |1    |null    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Low      |null |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-03.csv|\n",
      "|Medium   |1    |5.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Low      |0    |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Low      |0    |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Low      |0    |2.3  |null    |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|null     |1    |3.6  |null    |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|High     |1    |5.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Low      |0    |5.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-04.csv|\n",
      "|Medium   |0    |5.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Low      |0    |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Low      |1    |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|High     |0    |2.3  |null    |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Low      |1    |3.6  |null    |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|null     |0    |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Low      |0    |5.0  |null    |file:///C:/hdaoopdata/ica_source/2021-01-05.csv|\n",
      "|Medium   |puvi |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-07.csv|\n",
      "|Low      |raja |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-07.csv|\n",
      "|Low      |tamil|null |null    |file:///C:/hdaoopdata/ica_source/2021-01-07.csv|\n",
      "|High     |null |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-07.csv|\n",
      "|Low      |null |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-07.csv|\n",
      "|null     |null |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-07.csv|\n",
      "|Low      |null |null |null    |file:///C:/hdaoopdata/ica_source/2021-01-07.csv|\n",
      "+---------+-----+-----+--------+-----------------------------------------------+\n",
      "\n",
      "+----------+---------+------------------+\n",
      "|date      |Intensity|Mean_value        |\n",
      "+----------+---------+------------------+\n",
      "|2021-01-01|High     |5.0               |\n",
      "|2021-01-01|Low      |3.65              |\n",
      "|2021-01-01|Medium   |3.3333333333333335|\n",
      "|2021-01-02|High     |0.5               |\n",
      "|2021-01-02|Low      |1.0               |\n",
      "|2021-01-02|Medium   |0.0               |\n",
      "|2021-01-03|High     |1.0               |\n",
      "|2021-01-03|Low      |1.0               |\n",
      "|2021-01-03|Medium   |0.3333333333333333|\n",
      "|2021-01-04|null     |3.6               |\n",
      "|2021-01-04|High     |5.0               |\n",
      "|2021-01-04|Low      |3.65              |\n",
      "|2021-01-04|Medium   |5.0               |\n",
      "|2021-01-05|null     |null              |\n",
      "|2021-01-05|High     |2.3               |\n",
      "|2021-01-05|Low      |4.3               |\n",
      "|2021-01-05|Medium   |5.0               |\n",
      "|2021-01-06|null     |null              |\n",
      "|2021-01-06|High     |2.3               |\n",
      "|2021-01-06|Low      |4.3               |\n",
      "|2021-01-06|Medium   |5.0               |\n",
      "|2021-01-07|null     |null              |\n",
      "|2021-01-07|High     |null              |\n",
      "|2021-01-07|Low      |null              |\n",
      "|2021-01-07|Medium   |null              |\n",
      "+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "def filename(path):\n",
    "    return path\n",
    "\n",
    "sourceFile = udf(filename, StringType())\n",
    "\n",
    "df = spark.read.options(header='True',delimiter=',',inferSchema='True').\\\n",
    "    csv(\"file:///C:/hdaoopdata/ica_source/\")\n",
    "dff = df.select(df['*'],(sourceFile(input_file_name()))).withColumnRenamed(\"filename(input_file_name())\",\"date\")\n",
    "dff.show(truncate=False,n=100)  \n",
    "\n",
    "dff = dff.withColumn('Month', regexp_extract(dff['date'], '([0-9][0-9][0-9][0-9])-([0-9][0-9])-([0-9][0-9])', 2))\n",
    "dff = dff.withColumn('Year', regexp_extract(dff['date'], '([0-9][0-9][0-9][0-9])-([0-9][0-9])-([0-9][0-9])', 1))\n",
    "dff = dff.withColumn('Day', regexp_extract(dff['date'], '([0-9][0-9][0-9][0-9])-([0-9][0-9])-([0-9][0-9])', 3))\n",
    "\n",
    "df3=dff.select(concat_ws('-',dff.Year,dff.Month,dff.Day)\n",
    "              .alias(\"date\"),col(\"Intensity\"),col(\"seq\"),col(\"value\")) \n",
    "#.show()\n",
    "\n",
    "\n",
    "temp_avg = df3.groupby('date', 'Intensity')\\\n",
    "            .agg(avg('value').alias('Mean_value'))\\\n",
    "           .orderBy(col('date'), col('Intensity')).cache()\n",
    "temp_avg.show(truncate=False,n=100)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark --packages org.postgresql:postgresql:42.2.10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+----------------+\n",
      "|no_offer_received|no_offer_redem|         cust_id|\n",
      "+-----------------+--------------+----------------+\n",
      "|                4|             2| 346495553236295|\n",
      "|                4|             0|4716095485621384|\n",
      "|                4|             4|4716744331063073|\n",
      "|                4|             0|4929800060892364|\n",
      "|                4|             4|5370009829177288|\n",
      "+-----------------+--------------+----------------+\n",
      "\n",
      "+----------------+-------------------+---+-----------------+--------------+------+-----------+---+---+---+---+---+\n",
      "|         cust_id|          cust_name|age|no_offer_received|no_offer_redem|visits|total_sales|Sat|Sun|Thu|Tue|Wed|\n",
      "+----------------+-------------------+---+-----------------+--------------+------+-----------+---+---+---+---+---+\n",
      "|4716744331063073|Dr. Carley Predovic| 37|                4|             4|     1|          8|  0|  0|  4|  0|  0|\n",
      "|5370009829177288|      Lilla Weimann| 28|                4|             4|     1|         15|  0|  4|  0|  0|  0|\n",
      "|5370009829177288|      Lilla Weimann| 28|                4|             4|     1|          5|  0|  0|  0|  0|  4|\n",
      "|4716744331063073|Dr. Carley Predovic| 37|                4|             4|     1|         18|  0|  4|  0|  0|  0|\n",
      "|4716095485621384|Tatum Rosenbaum Jr.| 29|                4|             0|     1|          1|  4|  0|  0|  0|  0|\n",
      "|4929800060892364|   Zula Ziemann PhD| 29|                4|             0|     1|          0|  0|  0|  0|  4|  0|\n",
      "|4929800060892364|   Zula Ziemann PhD| 29|                4|             0|     1|         10|  4|  0|  0|  0|  0|\n",
      "|4716095485621384|Tatum Rosenbaum Jr.| 29|                4|             0|     1|         11|  0|  0|  0|  4|  0|\n",
      "+----------------+-------------------+---+-----------------+--------------+------+-----------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "dfc = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"cust_info_s\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"puvi\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "#dfc.show()\n",
    "\n",
    "\n",
    "\n",
    "dft = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"trans_info_s\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"puvi\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "#dft.show()\n",
    "\n",
    "dfo = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"offer_info_s\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"puvi\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "#dfo.show()\n",
    "\n",
    "\n",
    "dfc.registerTempTable('cust_info')\n",
    "dft.registerTempTable('trans_info')\n",
    "dfo.registerTempTable('offer_info')\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "\n",
    "#sqlc.sql(\"select * from cust_info\").show(10)\n",
    "#sqlc.sql(\"select * from trans_info\").show(10)\n",
    "#sqlc.sql(\"select * from offer_info\").show(10)\n",
    "\n",
    "#sqlContext.sql(\"select to_date('2017-01-01') as time, date_format(to_date('2017-01-01'),'W') as week\").show()\n",
    "\n",
    "#sqlContext.sql(\"\"\"select date_format(date (b.date),'E') as days, \n",
    "#                date_format(to_date(b.date),'W') as week, \n",
    "#                int(datediff(current_date(),TO_DATE(CAST(UNIX_TIMESTAMP(b.date,'yyyy-MM-dd') AS TIMESTAMP)))/365) as age\n",
    "#                from trans_info b \n",
    "#                \"\"\").show()\n",
    "\n",
    "sqlContext.sql(\"\"\"select count(o.offer_id) as no_offer_received,sum(offer_redem) as no_offer_redem,o.cust_id \n",
    "            from offer_info o\n",
    "            inner join trans_info t\n",
    "            on t.cust_id = o.cust_id\n",
    "            group by o.cust_id order by o.cust_id\n",
    "                  \"\"\").show()\n",
    "\n",
    "df = sqlc.sql(\"\"\"  WITH sales_data AS ( select sum(sales) as total_sales,count(trans_id) as visits,cust_id,date \n",
    "            from trans_info\n",
    "            group by date,cust_id),\n",
    "            offer_data AS ( \n",
    "            select count(o.offer_id) as no_offer_received,sum(offer_redem) as no_offer_redem,o.cust_id \n",
    "            from offer_info o\n",
    "            inner join trans_info t\n",
    "            on t.cust_id = o.cust_id\n",
    "            group by o.cust_id order by o.cust_id)\n",
    "            select b.date,b.cust_id,a.cust_name,a.cust_dob,\n",
    "            int(datediff(current_date(),TO_DATE(CAST(UNIX_TIMESTAMP(a.cust_dob,'yyyy-MM-dd') AS TIMESTAMP)))/365) as age,\n",
    "            total_sales,c.no_offer_received,c.no_offer_redem,visits, date_format(to_date(b.date),'E') as days,\n",
    "            int(date_format(to_date(b.date),'w')) AS  week_number\n",
    "            from cust_info a\n",
    "            left join sales_data b\n",
    "            on a.cust_id = b.cust_id\n",
    "            inner join offer_data c\n",
    "            on b.cust_id = c.cust_id\n",
    "            group by b.date,a.cust_id,b.cust_id,a.cust_name,a.cust_dob,c.no_offer_received,c.no_offer_redem,total_sales,visits,days\n",
    "            order by a.cust_id\n",
    "            \"\"\")\n",
    "\n",
    "#df.show()\n",
    "\n",
    "#reshaped_df = df.select(\"date\",\"cust_id\",\"cust_name\",\"cust_dob\",\"age\",\"total_sales\",\"no_offer_received\",\"no_offer_redem\",\"visits\",\"days\",\"week_number\").groupby('cust_id').pivot('days').max('no_offer_received').fillna(0)\n",
    "\n",
    "reshaped_dfs = df.groupby('cust_id','cust_name','age','no_offer_received','no_offer_redem','visits','total_sales').pivot('days').max('no_offer_received').fillna(0)\n",
    "\n",
    "reshaped_dfs.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+---+----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "|         cust_id|          cust_name|age|      date|days|no_offer_received|no_offer_redem|total_sales|Sat|Thu|Wed|\n",
      "+----------------+-------------------+---+----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "|4716095485621384|Tatum Rosenbaum Jr.| 29|1987-02-07| Sat|                2|             0|          1|  2|  0|  0|\n",
      "|4716744331063073|Dr. Carley Predovic| 37|1987-09-17| Thu|                2|             2|          8|  0|  2|  0|\n",
      "|4929800060892364|   Zula Ziemann PhD| 29|1995-09-09| Sat|                2|             0|         10|  2|  0|  0|\n",
      "|5370009829177288|      Lilla Weimann| 28|2005-08-24| Wed|                2|             2|          5|  0|  0|  2|\n",
      "+----------------+-------------------+---+----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Data extraction from Postgres server for dependent tables\n",
    "\n",
    "df_extract_cus_info = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"cust_info_s\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"puvi\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "\n",
    "df_extract_trans_info = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"trans_info_s\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"puvi\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df_extract_offer_info = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"offer_info_s\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"puvi\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "# Storing the source Postgres dependent tables into temp tables\n",
    "\n",
    "\n",
    "df_extract_cus_info.registerTempTable('cust_info')\n",
    "df_extract_trans_info.registerTempTable('trans_info')\n",
    "df_extract_offer_info.registerTempTable('offer_info')\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "# Tranforming the data into business requirements\n",
    "\n",
    "df_transf = sqlc.sql(\"\"\"  WITH sales_data AS ( select sum(sales) as total_sales,count(trans_id) as visits,cust_id,date \n",
    "            from trans_info\n",
    "            group by date,cust_id),\n",
    "            offer_data AS ( \n",
    "            select count(o.offer_id) as no_offer_received,sum(offer_redem) as no_offer_redem,o.cust_id \n",
    "            from offer_info o\n",
    "            inner join trans_info t\n",
    "            on t.cust_id = o.cust_id\n",
    "            group by o.cust_id order by o.cust_id)\n",
    "            select b.date,b.cust_id,a.cust_name,a.cust_dob,\n",
    "            int(datediff(current_date(),TO_DATE(CAST(UNIX_TIMESTAMP(a.cust_dob,'yyyy-MM-dd') AS TIMESTAMP)))/365) as age,\n",
    "            total_sales,c.no_offer_received,c.no_offer_redem,visits, date_format(to_date(b.date),'E') as days,\n",
    "            int(date_format(to_date(b.date),'w')) AS  week_number\n",
    "            from cust_info a\n",
    "            left join sales_data b\n",
    "            on a.cust_id = b.cust_id\n",
    "            inner join offer_data c\n",
    "            on b.cust_id = c.cust_id\n",
    "            group by b.date,a.cust_id,b.cust_id,a.cust_name,a.cust_dob,c.no_offer_received,c.no_offer_redem,total_sales,visits,days\n",
    "            order by a.cust_id\n",
    "            \"\"\")\n",
    "\n",
    "final_dfs = df_transf.orderBy('cust_id').groupby('cust_id','cust_name','age','date','days','no_offer_received','no_offer_redem','total_sales').pivot('days').max('no_offer_received').fillna(0)\n",
    "\n",
    "#Sampling data - In case of job failure data can be view in Spark UI\n",
    "\n",
    "final_dfs.show()\n",
    "\n",
    "\n",
    "# Writin the final data into CSV files\n",
    "\n",
    "\n",
    "final_dfs.repartition(1).write.mode(\"overwrite\").option(\"header\",True).csv(\"file:///C:/hdaoopdata/ics_weekly_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------+--------+\n",
      "|          cust_name|         cust_id|  cust_dob|  gender|\n",
      "+-------------------+----------------+----------+--------+\n",
      "|         RoseJacobi|6011837504027367|1981-08-06|   Edgar|\n",
      "|Dr. Carley Predovic|4716744331063073|1984-07-30|   Deron|\n",
      "|      Lilla Weimann|5370009829177288|1993-05-16|Leonardo|\n",
      "|Cathrine Hodkiewicz| 345858730400343|1995-05-20|    Rick|\n",
      "|      Celia Pfeffer| 340364615541355|2009-06-25|  Bryana|\n",
      "|        Cleve Fahey|2628327277351883|2017-08-16| Ibrahim|\n",
      "|     Gillian Senger| 346688355572616|1972-02-06| Keshawn|\n",
      "|     Joshua Roberts|5290632464794443|1995-08-18|   Ethan|\n",
      "|   Rasheed Gislason|2506922387299209|1994-07-06| Rasheed|\n",
      "|      Felicity Ryan|4532251518044506|2001-09-08|   Aleen|\n",
      "+-------------------+----------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+----------------+----------+--------+---------+-----------+-----+----------+\n",
      "|trans_id|         cust_id|product_id|store_id| offer_id|offer_redem|sales|      date|\n",
      "+--------+----------------+----------+--------+---------+-----------+-----+----------+\n",
      "|      29|4716744331063073|         5|       5|   162054|          1|    8|1987-09-17|\n",
      "|      25|4716095485621384|         5|       1|      860|          0|    1|1987-02-07|\n",
      "|      26|5370009829177288|         6|       8|   865856|          1|    5|2005-08-24|\n",
      "|      20| 346495553236295|         8|       7|977569237|          1|    7|2008-10-15|\n",
      "|      28| 340364615541355|         9|       2|       56|          0|    9|2006-10-28|\n",
      "|      27|2628327277351883|         1|       7|      146|          1|    3|1978-12-27|\n",
      "|      27| 346688355572616|         6|       7|     9090|          0|    4|2003-06-14|\n",
      "|      29|5290632464794443|         7|       9|    57620|          0|    4|2004-04-29|\n",
      "|      28|2506922387299209|         2|       7|       58|          0|    7|1992-07-22|\n",
      "|      25|4532251518044506|         9|       7|   361328|          0|    9|1975-03-26|\n",
      "+--------+----------------+----------+--------+---------+-----------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+----------------+---------+----------+----------+\n",
      "|offer_name|         cust_id| offer_id|start_date|  end_date|\n",
      "+----------+----------------+---------+----------+----------+\n",
      "|  BUYOGETO|4716744331063073|   162054|2020-05-21|2021-06-21|\n",
      "| SUMMEROFF|4716095485621384|      860|2014-11-05|2028-05-23|\n",
      "|  CHRISOFF|5370009829177288|   865856|1982-10-15|2021-11-11|\n",
      "|  BLACKOFF| 346495553236295|977569237|1994-12-18|2041-02-28|\n",
      "|    WEDOFF|4929800060892364|    20681|1972-05-18|2023-10-31|\n",
      "| SUMMEROFF|5474205046620622|  7554030|1982-09-24|1986-11-05|\n",
      "|  BUYOGETO|2432018555321971| 63791287|2020-03-09|1997-02-12|\n",
      "|  CHRISOFF|   4929389456522|      870|2004-05-04|1971-02-02|\n",
      "|  BUYOGETO|4024007158401561|       57|1989-08-25|1994-09-09|\n",
      "|  CHRISOFF|6011702932606440|   855528|2011-02-24|1998-12-20|\n",
      "+----------+----------------+---------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+-------------------+---+----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "|         cust_id|          cust_name|age|      date|days|no_offer_received|no_offer_redem|total_sales|Sat|Thu|Wed|\n",
      "+----------------+-------------------+---+----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "|4716095485621384|Tatum Rosenbaum Jr.| 29|1987-02-07| Sat|                2|             0|          1|  2|  0|  0|\n",
      "|4716744331063073|Dr. Carley Predovic| 37|1987-09-17| Thu|                2|             2|          8|  0|  2|  0|\n",
      "|4929800060892364|   Zula Ziemann PhD| 29|1995-09-09| Sat|                2|             0|         10|  2|  0|  0|\n",
      "|5370009829177288|      Lilla Weimann| 28|2005-08-24| Wed|                2|             2|          5|  0|  0|  2|\n",
      "+----------------+-------------------+---+----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Temptables cleaning process\n",
    "\n",
    "spark.catalog.dropTempView(\"cust_info_s\")\n",
    "spark.catalog.dropTempView(\"trans_info_s\")\n",
    "spark.catalog.dropTempView(\"offer_info_s\")\n",
    "\n",
    "# List of table to extract from source server\n",
    "\n",
    "tablename_list = ['cust_info_s','trans_info_s','offer_info_s']\n",
    "\n",
    "# Data extraction from Postgres server for dependent tables\n",
    "    \n",
    "url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "reader = (\n",
    "    sqlContext.read.format(\"jdbc\")\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", \"postgres\")\n",
    "    .option(\"password\", \"puvi\")\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    ")\n",
    "for tablename in tablename_list:\n",
    "    reader.option(\"dbtable\", tablename).load().registerTempTable(tablename)\n",
    "\n",
    "\n",
    "    \n",
    "# Sample view of source tables extracted from Postgres server   \n",
    "\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "sqlc.sql(\"select * from cust_info_s\").show(10)\n",
    "sqlc.sql(\"select * from trans_info_s\").show(10)\n",
    "sqlc.sql(\"select * from offer_info_s\").show(10)\n",
    "\n",
    "# Tranforming the data into business requirements\n",
    "\n",
    "df_transf = sqlc.sql(\"\"\"  WITH sales_data AS ( select sum(sales) as total_sales,count(trans_id) as visits,cust_id,date \n",
    "            from trans_info_s\n",
    "            group by date,cust_id),\n",
    "            offer_data AS ( \n",
    "            select count(o.offer_id) as no_offer_received,sum(offer_redem) as no_offer_redem,o.cust_id \n",
    "            from offer_info_s o\n",
    "            inner join trans_info_s t\n",
    "            on t.cust_id = o.cust_id\n",
    "            group by o.cust_id order by o.cust_id)\n",
    "            select b.date,b.cust_id,a.cust_name,a.cust_dob,\n",
    "            int(datediff(current_date(),TO_DATE(CAST(UNIX_TIMESTAMP(a.cust_dob,'yyyy-MM-dd') AS TIMESTAMP)))/365) as age,\n",
    "            total_sales,c.no_offer_received,c.no_offer_redem,visits, date_format(to_date(b.date),'E') as days,\n",
    "            int(date_format(to_date(b.date),'w')) AS  week_number\n",
    "            from cust_info_s a\n",
    "            left join sales_data b\n",
    "            on a.cust_id = b.cust_id\n",
    "            inner join offer_data c\n",
    "            on b.cust_id = c.cust_id\n",
    "            group by b.date,a.cust_id,b.cust_id,a.cust_name,a.cust_dob,c.no_offer_received,c.no_offer_redem,total_sales,visits,days\n",
    "            order by a.cust_id\n",
    "            \"\"\")\n",
    "\n",
    "final_dfs = df_transf.orderBy('cust_id').groupby('cust_id','cust_name','age','date','days','no_offer_received','no_offer_redem','total_sales').pivot('days').max('no_offer_received').fillna(0)\n",
    "\n",
    "#Sampling data - In case of job failure data can be viewed in Spark UI\n",
    "\n",
    "final_dfs.show()\n",
    "\n",
    "\n",
    "# Writin the final data into CSV files\n",
    "\n",
    "\n",
    "final_dfs.repartition(1).write.mode(\"overwrite\").option(\"header\",True).csv(\"file:///C:/hdaoopdata/ics_weekly_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
