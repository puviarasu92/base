{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Importing Postgres JDBC packages for data extraction\n",
    "pyspark --packages org.postgresql:postgresql:42.2.10 --conf spark.sql.catalogImplementation=in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------+--------+\n",
      "|          cust_name|         cust_id|  cust_dob|  gender|\n",
      "+-------------------+----------------+----------+--------+\n",
      "|         RoseJacobi|6011837504027367|1981-08-06|   Edgar|\n",
      "|Dr. Carley Predovic|4716744331063073|1984-07-30|   Deron|\n",
      "|      Lilla Weimann|5370009829177288|1993-05-16|Leonardo|\n",
      "|Cathrine Hodkiewicz| 345858730400343|1995-05-20|    Rick|\n",
      "|      Celia Pfeffer| 340364615541355|2009-06-25|  Bryana|\n",
      "|        Cleve Fahey|2628327277351883|2017-08-16| Ibrahim|\n",
      "|     Gillian Senger| 346688355572616|1972-02-06| Keshawn|\n",
      "|     Joshua Roberts|5290632464794443|1995-08-18|   Ethan|\n",
      "|   Rasheed Gislason|2506922387299209|1994-07-06| Rasheed|\n",
      "|      Felicity Ryan|4532251518044506|2001-09-08|   Aleen|\n",
      "+-------------------+----------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+----------------+----------+--------+---------+-----------+-----+----------+\n",
      "|trans_id|         cust_id|product_id|store_id| offer_id|offer_redem|sales|      date|\n",
      "+--------+----------------+----------+--------+---------+-----------+-----+----------+\n",
      "|      29|4716744331063073|         5|       5|   162054|          1|    8|1987-09-17|\n",
      "|      25|4716095485621384|         5|       1|      860|          0|    1|1987-02-07|\n",
      "|      26|5370009829177288|         6|       8|   865856|          1|    5|2005-08-24|\n",
      "|      20| 346495553236295|         8|       7|977569237|          1|    7|2008-10-15|\n",
      "|      28| 340364615541355|         9|       2|       56|          0|    9|2006-10-28|\n",
      "|      27|2628327277351883|         1|       7|      146|          1|    3|1978-12-27|\n",
      "|      27| 346688355572616|         6|       7|     9090|          0|    4|2003-06-14|\n",
      "|      29|5290632464794443|         7|       9|    57620|          0|    4|2004-04-29|\n",
      "|      28|2506922387299209|         2|       7|       58|          0|    7|1992-07-22|\n",
      "|      25|4532251518044506|         9|       7|   361328|          0|    9|1975-03-26|\n",
      "+--------+----------------+----------+--------+---------+-----------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+----------------+---------+----------+----------+\n",
      "|offer_name|         cust_id| offer_id|start_date|  end_date|\n",
      "+----------+----------------+---------+----------+----------+\n",
      "|  BUYOGETO|4716744331063073|   162054|2020-05-21|2021-06-21|\n",
      "| SUMMEROFF|4716095485621384|      860|2014-11-05|2028-05-23|\n",
      "|  CHRISOFF|5370009829177288|   865856|1982-10-15|2021-11-11|\n",
      "|  BLACKOFF| 346495553236295|977569237|1994-12-18|2041-02-28|\n",
      "|    WEDOFF|4929800060892364|    20681|1972-05-18|2023-10-31|\n",
      "| SUMMEROFF|5474205046620622|  7554030|1982-09-24|1986-11-05|\n",
      "|  BUYOGETO|2432018555321971| 63791287|2020-03-09|1997-02-12|\n",
      "|  CHRISOFF|   4929389456522|      870|2004-05-04|1971-02-02|\n",
      "|  BUYOGETO|4024007158401561|       57|1989-08-25|1994-09-09|\n",
      "|  CHRISOFF|6011702932606440|   855528|2011-02-24|1998-12-20|\n",
      "+----------+----------------+---------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+-------------------+---+-----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "|         cust_id|          cust_name|age|week_number|days|no_offer_received|no_offer_redem|total_sales|Sat|Thu|Wed|\n",
      "+----------------+-------------------+---+-----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "|4716095485621384|Tatum Rosenbaum Jr.| 29|          6| Sat|                2|             0|          1|  2|  0|  0|\n",
      "|4716744331063073|Dr. Carley Predovic| 37|         38| Thu|                2|             2|          8|  0|  2|  0|\n",
      "|4929800060892364|   Zula Ziemann PhD| 29|         36| Sat|                2|             0|         10|  2|  0|  0|\n",
      "|5370009829177288|      Lilla Weimann| 28|         35| Wed|                2|             2|          5|  0|  0|  2|\n",
      "+----------------+-------------------+---+-----------+----+-----------------+--------------+-----------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Sparkusecase') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Temptables cleaning process\n",
    "\n",
    "spark.catalog.dropTempView(\"cust_info_s\")\n",
    "spark.catalog.dropTempView(\"trans_info_s\")\n",
    "spark.catalog.dropTempView(\"offer_info_s\")\n",
    "\n",
    "# List of table to extract from source server\n",
    "\n",
    "tablename_list = ['cust_info_s','trans_info_s','offer_info_s']\n",
    "\n",
    "# Data extraction from Postgres server for dependent tables\n",
    "    \n",
    "url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "reader = (\n",
    "    sqlContext.read.format(\"jdbc\")\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", \"postgres\")\n",
    "    .option(\"password\", \"puvi\")\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    ")\n",
    "for tablename in tablename_list:\n",
    "    reader.option(\"dbtable\", tablename).load().registerTempTable(tablename)\n",
    "\n",
    "\n",
    "    \n",
    "# Sample view of source tables extracted from Postgres server   \n",
    "\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "sqlc.sql(\"select * from cust_info_s\").show(10)\n",
    "sqlc.sql(\"select * from trans_info_s\").show(10)\n",
    "sqlc.sql(\"select * from offer_info_s\").show(10)\n",
    "\n",
    "# Tranforming the data into business requirements\n",
    "\n",
    "df_transf = sqlc.sql(\"\"\"  WITH sales_data AS ( select sum(sales) as total_sales,count(trans_id) as visits,cust_id,date \n",
    "            from trans_info_s\n",
    "            group by date,cust_id),\n",
    "            offer_data AS ( \n",
    "            select count(o.offer_id) as no_offer_received,sum(offer_redem) as no_offer_redem,o.cust_id \n",
    "            from offer_info_s o\n",
    "            inner join trans_info_s t\n",
    "            on t.cust_id = o.cust_id\n",
    "            group by o.cust_id order by o.cust_id)\n",
    "            select b.date,b.cust_id,a.cust_name,a.cust_dob,\n",
    "            int(datediff(current_date(),TO_DATE(CAST(UNIX_TIMESTAMP(a.cust_dob,'yyyy-MM-dd') AS TIMESTAMP)))/365) as age,\n",
    "            total_sales,c.no_offer_received,c.no_offer_redem,visits, date_format(to_date(b.date),'E') as days,\n",
    "            int(date_format(to_date(b.date),'w')) AS  week_number\n",
    "            from cust_info_s a\n",
    "            left join sales_data b\n",
    "            on a.cust_id = b.cust_id\n",
    "            inner join offer_data c\n",
    "            on b.cust_id = c.cust_id\n",
    "            group by b.date,a.cust_id,b.cust_id,a.cust_name,a.cust_dob,c.no_offer_received,c.no_offer_redem,total_sales,visits,days\n",
    "            order by a.cust_id\n",
    "            \"\"\")\n",
    "\n",
    "final_dfs = df_transf.orderBy('cust_id').groupby('cust_id','cust_name','age','week_number','no_offer_received','no_offer_redem','total_sales').pivot('days').max('no_offer_received').fillna(0)\n",
    "\n",
    "#Sampling data - In case of job failure data can be viewed in Spark UI\n",
    "\n",
    "final_dfs.show()\n",
    "\n",
    "\n",
    "# Writin the final data into CSV files\n",
    "\n",
    "\n",
    "final_dfs.repartition(1).write.mode(\"overwrite\").option(\"header\",True).csv(\"file:///C:/hdaoopdata/ics_weekly_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Check for trans date and check the peroid whether its falls on valid range on offer table\n",
    "Visit on days level - instead of no of received , \n",
    "Convert spark sql into dataframe\n",
    "ETL framwork design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+---+-----------+-----------------+--------------+--------------+-----------+------+---+---+---+---+\n",
      "|         cust_id|          cust_name|age|week_number|no_offer_received|no_offer_redem|offer_validity|total_sales|visits|Sat|Sun|Thu|Wed|\n",
      "+----------------+-------------------+---+-----------+-----------------+--------------+--------------+-----------+------+---+---+---+---+\n",
      "|4716095485621384|Tatum Rosenbaum Jr.| 29|          6|                6|             4|             1|         13|     3|  0|  3|  0|  0|\n",
      "|4716744331063073|Dr. Carley Predovic| 37|         38|                2|             2|             0|          8|     1|  0|  0|  1|  0|\n",
      "|4929800060892364|   Zula Ziemann PhD| 29|         36|                2|             0|             1|         10|     1|  1|  0|  0|  0|\n",
      "|5370009829177288|      Lilla Weimann| 28|         34|                2|             2|             1|          5|     1|  0|  0|  0|  1|\n",
      "+----------------+-------------------+---+-----------+-----------------+--------------+--------------+-----------+------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import covar_pop\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import weekofyear\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Sparkusecase') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Temptables cleaning process\n",
    "\n",
    "spark.catalog.dropTempView(\"cust_info_s\")\n",
    "spark.catalog.dropTempView(\"trans_info_s\")\n",
    "spark.catalog.dropTempView(\"offer_info_s\")\n",
    "\n",
    "# List of table to extract from source server\n",
    "\n",
    "tablename_list = ['cust_info_s','trans_info_s','offer_info_s']\n",
    "\n",
    "# Data extraction from Postgres server for dependent tables\n",
    "    \n",
    "url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "reader = (\n",
    "    sqlContext.read.format(\"jdbc\")\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", \"postgres\")\n",
    "    .option(\"password\", \"puvi\")\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    ")\n",
    "for tablename in tablename_list:\n",
    "    reader.option(\"dbtable\", tablename).load().registerTempTable(tablename)\n",
    "\n",
    "\n",
    "    \n",
    "# Sample view of source tables extracted from Postgres server   \n",
    "\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "#sqlc.sql(\"select * from cust_info_s\").show(10)\n",
    "#sqlc.sql(\"select * from trans_info_s\").show(10)\n",
    "#sqlc.sql(\"select * from offer_info_s\").show(10)\n",
    "\n",
    "# Tranforming the data into business requirements\n",
    "\n",
    "df_trans_info = sqlc.sql(\"select * from trans_info_s\")\n",
    "df_offer_info = sqlc.sql(\"select * from offer_info_s\")\n",
    "df_cust_info = sqlc.sql(\"select DATE(cust_dob),cust_id,cust_name from cust_info_s\")\n",
    "\n",
    "#df_cust_info.printSchema()\n",
    "\n",
    "df_sales_data = df_trans_info.orderBy('cust_id').groupBy('date','cust_id').agg(sum('sales'),count('trans_id')) \\\n",
    ".withColumnRenamed('sum(sales)', 'total_sales').withColumnRenamed('count(trans_id)', 'visits')\n",
    "\n",
    "#df_sales_data.show()\n",
    "\n",
    "df_offer_data_f = df_offer_info.join(df_trans_info,df_offer_info.cust_id ==  df_trans_info.cust_id,'inner') \\\n",
    ".orderBy(df_offer_info.cust_id).groupBy(df_offer_info.cust_id).agg(sum('offer_redem'),count(df_offer_info.offer_id)) \\\n",
    ".withColumnRenamed('sum(offer_redem)', 'no_offer_redem').withColumnRenamed('count(offer_id)', 'no_offer_received') \n",
    "\n",
    "#df_offer_data_f.show(truncate=False)\n",
    "\n",
    "#Check trans date is lies in peroid between offer start and end date - if yes offer is redemed and 1, else 0\n",
    "\n",
    "df_off = df_trans_info.join(df_offer_info,df_trans_info.cust_id ==  df_offer_info.cust_id,'inner') \\\n",
    ".select(df_trans_info.cust_id,df_trans_info.date,df_offer_info.start_date,df_offer_info.end_date,df_offer_info.offer_id,df_trans_info.offer_redem,\n",
    "F.when((df_trans_info.date >= df_offer_info.start_date) & (df_trans_info.date <= df_offer_info.end_date), 1) \\\n",
    ".otherwise(0).alias('offer_validity'))\n",
    "\n",
    "#df_off.show()\n",
    "\n",
    "\n",
    "df_offer_data_check = df_off.orderBy(df_off.cust_id,) \\\n",
    ".groupBy(df_off.cust_id,df_off.offer_validity).agg(sum('offer_redem'),count('offer_id')) \\\n",
    ".withColumnRenamed('sum(offer_redem)', 'no_offer_redem').withColumnRenamed('count(offer_id)', 'no_offer_received') \n",
    "\n",
    "#df_offer_data.show(truncate=False)\n",
    "\n",
    "df_offer_data = df_offer_data_check.withColumnRenamed(\"cust_id\", \"off_cust_id\")\n",
    "\n",
    "#df_offer_data.show(truncate=False)\n",
    "\n",
    "df_trans_data = df_cust_info.join(df_sales_data,df_cust_info.cust_id ==  df_sales_data.cust_id,'left') \\\n",
    ".join(df_offer_data,df_sales_data.cust_id ==  df_offer_data.off_cust_id,'inner') \\\n",
    ".select(df_sales_data.date,df_sales_data.cust_id,df_cust_info.cust_name,df_cust_info.cust_dob \\\n",
    ",df_sales_data.total_sales,df_offer_data.no_offer_received,df_offer_data.no_offer_redem,df_offer_data.offer_validity,df_sales_data.visits \\\n",
    ",round(months_between(current_date(),df_cust_info.cust_dob)/lit(12),2).cast('int').alias('age') \\\n",
    ",date_format(to_date(df_sales_data.date),'E').alias('days') \\\n",
    ",weekofyear(df_sales_data.date).alias('week_number')) \n",
    "\n",
    "#Derive which day the coustomer spends more\n",
    "\n",
    "final_dfs = df_trans_data.orderBy('cust_id').groupby('cust_id','cust_name','age','week_number','no_offer_received','no_offer_redem','offer_validity','total_sales','visits').pivot('days').max('visits').fillna(0)\n",
    "\n",
    "\n",
    "#Sampling data - In case of job failure data can be viewed in Spark UI\n",
    "\n",
    "final_dfs.show()\n",
    "\n",
    "# Writin the final data into CSV files\n",
    "\n",
    "final_dfs.repartition(1).write.mode(\"overwrite\").option(\"header\",True).csv(\"file:///C:/hdaoopdata/ics_weekly_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
